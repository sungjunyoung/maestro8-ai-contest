{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 기반 상품 카테고리 자동 분류 서버 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 학습 데이터를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_list = []\n",
    "y_text_list = []\n",
    "enc = sys.getdefaultencoding()\n",
    "with open(\"refined_category_dataset.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "#         print (line)\n",
    "        info = json.loads(line.strip())\n",
    "        x_text_list.append((info['pid'],info['name']))\n",
    "        y_text_list.append(info['cate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(y_name_id_dict,\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 형식으로 되어 있는 카테고리 명을 숫자 id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name_id_dict = joblib.load(\"y_name_id_dict.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'반려동물': 4, '스포츠/레저': 9, '도서/문구': 8, '가구/인테리어': 11, '식품': 6, '건강': 7, '출산/육아': 16, '컴퓨터': 3, '의류': 2, '뷰티': 0, '여행/e쿠폰': 15, '가전': 12, '취미': 5, '생필품/주방': 13, '디지털': 10, '잡화': 14, '자동차/공구': 1}\n"
     ]
    }
   ],
   "source": [
    "print(y_name_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('의류', 0), ('반려동물', 15), ('건강', 1), ('식품', 13), ('생필품/주방', 2), ('스포츠/레저', 3), ('컴퓨터', 10), ('도서/문구', 6), ('취미', 9), ('디지털', 8), ('자동차/공구', 16), ('뷰티', 11), ('출산/육아', 12), ('가구/인테리어', 7), ('여행/e쿠폰', 14), ('잡화', 4), ('가전', 5)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_name_set = set(y_text_list)\n",
    "y_name_id_dict = dict(zip(y_name_set, range(len(y_name_set))))\n",
    "print(y_name_id_dict.items())\n",
    "y_id_name_dict = dict(zip(range(len(y_name_set)),y_name_set))\n",
    "y_list = [y_name_id_dict[x] for x in y_text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test 분리하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(x_text_list, y_list, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 기반 text 분류에 필요한 모듈 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import gensim\n",
    "import keras.preprocessing.text\n",
    "import numpy\n",
    "import gensim\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 파일을 만약 만들었다면, 아래와 같이 로드 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('/workspace/resources/11st_all_product_name.segment.0918.15w100e3min.model', binary=True)\n",
    "# word2vec.init_sims(replace=True)\n",
    "# word2vec으로 임베딩을 만들어서 로드하면 성능 올라감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text 데이터를 word-id 형태로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sequence_tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "sequence_tokenizer.fit_on_texts(map(lambda i : i[1], X_train))\n",
    "max_features = len(sequence_tokenizer.word_index)\n",
    "\n",
    "\n",
    "def texts_to_sequences2(d_list, tokenizer, maxlen=300):\n",
    "    seq = tokenizer.texts_to_sequences(d_list)\n",
    "    print('mean:', numpy.mean([len(x) for x in seq]))\n",
    "    print('std:', numpy.std([len(x) for x in seq]))\n",
    "    print('median:', numpy.median([len(x) for x in seq]))\n",
    "    print('max:', numpy.max([len(x) for x in seq]))\n",
    "    seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 7.78808823529\n",
      "std: 3.50519855834\n",
      "median: 7.0\n",
      "max: 28\n",
      "mean: 4.41352941176\n",
      "std: 3.05366595726\n",
      "median: 4.0\n",
      "max: 23\n"
     ]
    }
   ],
   "source": [
    "train = texts_to_sequences2(map(lambda i : i[1],X_train),sequence_tokenizer)\n",
    "test = texts_to_sequences2(map(lambda i : i[1],X_test),sequence_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word의 embedding 형태의 weight 를 초기화 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train.shape[1]\n",
    "\n",
    "input_tensor = keras.layers.Input(shape=(input_dim,), dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28627\n"
     ]
    }
   ],
   "source": [
    "word_vec_dim = 100\n",
    "not_ct = 0\n",
    "weights = numpy.zeros((max_features + 1, word_vec_dim))\n",
    "for word, index in sequence_tokenizer.word_index.items():\n",
    "    if False:\n",
    "        pass\n",
    "#     if word in word2vec.vocab:\n",
    "#         weights[index, :] = word2vec[word]\n",
    "    else:\n",
    "        not_ct+=1\n",
    "        weights[index, :] = numpy.random.uniform(-0.25, 0.25, word_vec_dim)\n",
    "# del word2vec\n",
    "# del sequence_tokenizer\n",
    "print (not_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  학습할 레이러를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = keras.layers.Embedding(input_dim=max_features + 1,\n",
    "                                  output_dim=word_vec_dim, input_length=input_dim,\n",
    "                                  weights=[weights],trainable=True)(input_tensor)\n",
    "# embedded2 = keras.layers.Dropout(0.9)(embedded)\n",
    "# embedded2 = embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=3, filters=50)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.4/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=298)`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.4/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=5, filters=50)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.4/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=296)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "tensors = []\n",
    "for filter_length in [3, 5]:\n",
    "    tensor = keras.layers.Convolution1D(nb_filter=50, filter_length=filter_length)(embedded)\n",
    "    tensor = keras.layers.Activation('relu')(tensor)\n",
    "    tensor = keras.layers.MaxPooling1D(pool_length=input_dim - filter_length + 1)(tensor)\n",
    "    tensor = keras.layers.Flatten()(tensor)\n",
    "    tensors.append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 300, 100)      2862800     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 298, 50)       15050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 296, 50)       25050       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 298, 50)       0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 296, 50)       0           conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 1, 50)         0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 1, 50)         0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 50)            0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 50)            0           max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 100)           0           flatten_3[0][0]                  \n",
      "                                                                   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 17)            1717        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,904,617\n",
      "Trainable params: 2,904,617\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/site-packages/ipykernel_launcher.py:2: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "/usr/local/lib/python3.4/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "# embedded = keras.layers.Dropout(0.5)(embedded)\n",
    "output_tensor = keras.layers.merge(tensors, mode='concat', concat_axis=1)\n",
    "# output_tensor = keras.layers.Dropout(0.5)(output_tensor) # 0.7312\n",
    "output_tensor = keras.layers.Dropout(0.5)(output_tensor) \n",
    "output_tensor = keras.layers.Dense(len(set(y_list)), activation='softmax')(output_tensor)\n",
    "\n",
    "# output = Dense(NUM_CLASSES, input_dim = hidden_dim_2, activation = \"softmax\")(pool_rnn) # See equations (6) and (7).\n",
    "\n",
    "cnn = keras.models.Model(input_tensor, output_tensor)\n",
    "cnn.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/10\n",
      "6800/6800 [==============================] - 26s - loss: 2.8716 - acc: 0.0687 - val_loss: 2.8082 - val_acc: 0.0800\n",
      "Epoch 2/10\n",
      "6800/6800 [==============================] - 25s - loss: 2.8090 - acc: 0.0940 - val_loss: 2.7879 - val_acc: 0.0994\n",
      "Epoch 3/10\n",
      "6800/6800 [==============================] - 25s - loss: 2.7479 - acc: 0.1299 - val_loss: 2.7702 - val_acc: 0.1194\n",
      "Epoch 4/10\n",
      "6800/6800 [==============================] - 24s - loss: 2.7131 - acc: 0.1418 - val_loss: 2.7563 - val_acc: 0.1353\n",
      "Epoch 5/10\n",
      "6800/6800 [==============================] - 24s - loss: 2.6765 - acc: 0.1674 - val_loss: 2.7406 - val_acc: 0.1500\n",
      "Epoch 6/10\n",
      "6800/6800 [==============================] - 23s - loss: 2.6367 - acc: 0.1918 - val_loss: 2.7251 - val_acc: 0.1582\n",
      "Epoch 7/10\n",
      "6800/6800 [==============================] - 20s - loss: 2.5951 - acc: 0.2126 - val_loss: 2.7086 - val_acc: 0.1824\n",
      "Epoch 8/10\n",
      "6800/6800 [==============================] - 21s - loss: 2.5568 - acc: 0.2390 - val_loss: 2.6916 - val_acc: 0.1947\n",
      "Epoch 9/10\n",
      "6800/6800 [==============================] - 20s - loss: 2.5043 - acc: 0.2760 - val_loss: 2.6728 - val_acc: 0.2071\n",
      "Epoch 10/10\n",
      "6800/6800 [==============================] - 20s - loss: 2.4656 - acc: 0.2843 - val_loss: 2.6544 - val_acc: 0.2212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9be7e522b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train, np.asarray(to_categorical(y_train)), batch_size=60, nb_epoch=10,\n",
    "        validation_data=(test, np.asarray(to_categorical(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_x_text_list = []\n",
    "with open(\"soma8_test_data.dat\",encoding=enc) as fin:\n",
    "    for line in fin.readlines():\n",
    "        info = json.loads(line.strip())\n",
    "        eval_x_text_list.append((info['pid'],info['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 4.48684210526\n",
      "std: 2.83126839769\n",
      "median: 4.0\n",
      "max: 18\n"
     ]
    }
   ],
   "source": [
    "eval_x_list = texts_to_sequences2(map(lambda i : i[1],eval_x_text_list),sequence_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_list = clf.predict(vectorizer.transform(map(lambda i : i[1],eval_x_text_list)))\n",
    "pred = cnn.predict(eval_x_list)\n",
    "pred_list = [argmax(y) for y in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.2476780185758514, 'msg': 'If you pull docker image before 2017-09-27 21:30,  pull your docker image again.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "name='성주녕'\n",
    "nickname='성주녕'\n",
    "mode='test'\n",
    "param = {'pred_list':\",\".join(map(lambda i : str(int(i)),pred_list)),\n",
    "         'name':name,'nickname':nickname,'mode':mode}\n",
    "d = requests.post('http://eval.buzzni.net:20001/eval',data=param)\n",
    "print (d.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 으로 추출한 이미지 데이터 사용하기 \n",
    " * 이 부분은 각자 한번 해보도록 해요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
